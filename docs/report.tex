\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{threeparttable}
\usepackage[hidelinks]{hyperref}

\begin{document}

\title{
  Management of Shared Mobility: Report
}

\author{
  \IEEEauthorblockN{Eduardo Passos}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202205630@up.pt
  }
  \and
  \IEEEauthorblockN{Guilherme Silva}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202205298@up.pt
  }
  \and
  \IEEEauthorblockN{Valentina Cadime}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202206262@up.pt
  }
}

\maketitle

\begin{abstract}
The following report involves the creation of a realistic simulation of an urban micro-mobility system, specialized for the management of shared electric scooter fleets. Its main purpose is to study different agents and respective policies under multiple scenarios, and, thus, analyze trade-offs between availability, waiting queues, and operational cost. The results indicate that Soft-Actor Critic (SAC) algorithm consistently delivers the best results across all scenarios. Nevertheless, its lack of interpretability may introduce risk and reduce transparency when implemented in the real world. Interpretable agents such as the Heuristic Agent (with manually crafted rules) and the Contextual Bandit (LinUCB) perform well, although they fail under demand heterogeneity and tend to sacrifice service quality, respectively. [PRECISO DE INFORMAÇÕES SOBRE O Episodic Multi-Armed Bandit]
% TODO:

\end{abstract}

\begin{IEEEkeywords}
Modelling, reinforcement learning, scooter fleet management, simulation
\end{IEEEkeywords}

\section{Introduction}
Shared electric micro-mobility systems operated by private companies have become common in urban environments. This trend is driven by the daily congestion, environmental pressures, and the limited flexibility of traditional public transportation. Electric scooters offer a compact alternative for short-distance travel, and their deployment does not require major infrastructure investments. However, the management of these fleets presents the challenge of keeping scooters continuously available, which leads to costs related to scooter redistribution and battery charging, both of which are strongly impacted by spatial and temporal demand imbalance.

In more detail, the variance in scooter demand is influenced by commute patterns, weather conditions, and special events. As a result, in certain situations, some stations may become saturated while others are empty, which causes a reduced scooter availability and poor service quality. This imbalance is further complicated by battery constraints and charging delays, which limit vehicle usability and introduce additional operational dependencies. Since fleet size, charging capacity, and relocation resources are finite, operators must constantly decide when and where to relocate and recharge the scooters. On the one hand, an excessive intervention will increase operational cost, but on the other hand, an insufficient intervention leads to unmet demand, longer waiting times, and revenue loss. Ultimately, the problem consists of selecting redistribution and charging actions over time that minimize operational cost while maintaining acceptable service quality under uncertainty and resource constraints.

Indeed, an ever-changing city is poorly suited to simple static planning, as manually crafted rules do not account for demand heterogeneity and stochastic external influences. Consequently, there is a strong need for systematic evaluation tools that allow the comparison of management strategies and the analysis of trade-offs between availability, waiting queues, and operational cost. Simulation provides a controlled and flexible environment to study these interactions, avoiding the risks and expenses of real-world experimentation.

The aim of this project is to build a realistic simulation of an urban shared electric scooter system for the evaluation of daily fleet management strategies. The primary goal is to minimize operating costs while maximizing service quantity and quality. To achieve this, the simulation models user demand, vehicle travel, battery depletion, charging processes, and operator-driven relocation actions, while incorporating stochastic exogenous factors such as weather and events. Multiple decision-making approaches are evaluated, including baseline policies, interpretable heuristic and bandit-based agents, and reinforcement learning methods such as Soft Actor-Critic (SAC).

The central research questions address how different management policies perform under heterogeneous and time-dependent demand, how battery and charging constraints affect system efficiency, and what trade-offs arise between service quality and operational cost. The underlying hypothesis is that learning-based agents achieve superior performance by adapting to complex demand patterns, while interpretable approaches provide greater transparency but may degrade under demand heterogeneity.

This report first describes the system model and simulation framework, followed by the decision-making agents and policies. The evaluation methodology and performance metrics are then presented, along with a comparative analysis of results across multiple scenarios. The report concludes with a discussion of key findings, limitations, and directions for future work.

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Context
    \item Problem statement. Clearly define the research question or technical challenge with a precise problem description and formal representations
    \item Motivation (to solve the problem) / significance
    \item Aim and goals
    \item Aim: the main expected outcome of this research project (the big picture!)
    \item Goals: specific objectives to be accomplished
    \item Research questions
    \item Hypotheses
    \item Document/paper structure
  \end{itemize}
}

\section{Related Work}

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Review of existing literature.
    \item Relevant concepts and studies.
    \item Summarize background and related work to highlight gaps addressed by the project.
    \item Discussion and critical analysis.
  \end{itemize}
}

The problem falls within the field of Urban Mobility and, more specifically, the subfield of Shared Micromobility and Fleet Management. It addresses how cities can efficiently manage and balance fleets of shared electric vehicles to meet fluctuating demand patterns throughout the day. As a result, and similarly to many of the modelling projects that seek to study the real world, this issue lies at the intersection of several disciplines.

From the perspective of transportation science, the problem concerns the modeling of human movement patterns and travel demand within urban environments. It involves understanding how people choose modes of transport, how their choices change with time of day, weather, or location, and how these behaviors influence the spatial distribution of shared vehicles.

In the area of optimization and operations research, the challenge translates into a set of complex decision problems. These include vehicle rebalancing (determining how and when to move vehicles from low-demand to high-demand areas), routing and scheduling of redistribution trucks, and the design of user incentives to encourage self-balancing behavior. Such problems are often formulated as variants of the Vehicle Routing Problem (VRP), capacitated network flow models, or stochastic optimization problems. Furthermore, the possibility of incorporating machine learning brings a predictive and adaptive layer to the system.


\section{Materials and Methods}

The system is formalized as an agent-based discrete-time simulation shaped by the interactions between a single operator (the agent), the entities, and finite network resources. The agent acts by controlling specific parameters that influence either scooter relocation and charging. The primary entities are the customers and the vehicle fleet. Customers are modeled as transient entities represented by stochastic arrival events, while the fleet consists of vehicle counts circulating between nodes. These entities interact with stations, which function as passive resources within the system. Stations are characterized by static attributes such as geographic location, parking capacity (slots), and charger availability, effectively constraining the flow of entities by limiting parking and charging capacities.

The simulation operates within a dynamic environment, whose specific instantiation for the experiments conducted is defined in \ref{tab:env-configs}, subject to external variations, including time-of-day effects, weather conditions, and special events. These environmental factors, combined with the stochastic user demand, constitute the \textit{uncontrollable exogenous variables}, corresponding to the probabilistic part of the system, as detailed in \ref{sec:sim_dynamics}.

The instantaneous status of the system is captured by state variables, which correspond to the \textit{endogenous variables} formally defined in \ref{sec:sim_state}. The evolution of this state is driven by customer arrivals, trip completions, and operator interventions, which trigger specific activities including vehicle rental, fleet relocation, and battery charging. The infrastructure parameters and resource limits that bound these activities form the \textit{controllable exogenous variables}, explored in \ref{sec:sim_config}. Additionally, the specific relocation and charging plans built by the operational policies at each time step are included in the controllable exogenous variables, as they are generated externally to the simulation dynamics, by the agent.

\subsection{\label{sec:sim_config}Simulation Configuration}
The simulation environment is characterized by a set of controllable exogenous parameters (implemented in the \texttt{SimConfig} dataclass).
\begin{itemize}
  \item $\Delta t$: Discrete simulation time step duration (minutes).
  \item $H$: Total simulation horizon (hours).
  \item $C \in \mathbb{N}^N$: Vector of station capacities, where $C_i$ is the maximum number of scooters allowed at station $i$.
  \item $\mathcal{T} \in \mathbb{R}^{N \times N}$: Travel time matrix, where $\tau_{ij}$ represents the effective transit time between station $i$ and $j$.
  \item $\mathcal{D} \in \mathbb{R}^{N \times N}$: Distance, where $d_{ij}$ represents the distance (km) between station $i$ and $j$.
  \item $K \in \mathbb{N}^N$: Vector of station chargers at each node.
  \item $\rho \in \mathbb{R}^N$: Charging rate vector, where $\rho_i$ denotes the SoC gain per hour at station $i$.
  % TODO: Are we sure this is total battery per vehicle? What does that mean?
  \item $B$: Total battery capacity per vehicle (kWh).
  \item $c_{energy}$: Unit cost of energy (€/kWh).
  \item $s_{min}$: Minimum SoC threshold required for a scooter to be eligible for rental. Always set to 0.15 (15\%).
  \item $\gamma_{res} \in \{0,1\}$: Operation policy parameter indicating if scooters connected to the grid should be reserved (unavailable for rental) during the active charging time step. For the experiments, this is always set to true.
\end{itemize}

\subsection{\label{sec:sim_state}Simulation State}
At any time $t$, the system state is defined by endogenous variables tracking the fleet and demand status (implemented as attributes of the \texttt{Sim} class):
\begin{itemize}
  \item $\mathbf{x}_t \in \mathbb{N}^N$: Vehicle count at each station.
  \item $\mathbf{s}_t \in [0,1]^N$: Average SoC at each station.
  \item $\mathbf{m}_t \in \mathbb{R}^N$: Aggregated energy "mass" at each station. Calculated as $m_{i,t} = x_{i,t} \cdot s_{i,t}$.
  \item $\mathbf{q}_t \in \mathbb{N}^N$: Number of users currently in queue.
  \item $\Psi_t$: Set of active trips currently in transit, implemented as a min-heap priority queue sorted by arrival time. A trip is a tuple $(t_{arrival}, j, \delta_{soc}, s_{depart})$, denoting arrival time, destination station, SoC usage, and departure SoC.
\end{itemize}

\subsection{\label{sec:sim_dynamics}Simulation Dynamics}

The state transition $\mathcal{S}_t \rightarrow \mathcal{S}_{t+1}$ occurs via the following sequential steps, in which the uncontrollable exogenous variables manifest as stochastic events:

\subsubsection{Demand and Departures}

Demand arrivals $\mathbf{A}_t$ follow a non-homogeneous Poisson process:
\begin{equation}
  \lambda_{i,t}^{eff} = \lambda_i \cdot f_{time}(t) \cdot f_{weather}(t) \cdot f_{event}(t)
\end{equation}
where $\lambda_i$ is the base arrival rate at station $i$, and $f_{(\cdot)} \in [0,1]$ model time-of-day, weather, and special event influences.

The number of served users is limited by the rentable stock of scooters ($s_{i,t} > s_{min}$).

User destinations are sampled from the origin-destination probability matrix $P \in \mathbb{R}^{N \times N}$, where $p_{ij}$ represents the probability of a user departing from station $i$ terminating their trip at station $j$. To simplify the simulation and focus on rebalancing dynamics rather than complex routing patterns, we assume a uniform distribution:
\begin{equation}
  p_{ij} = \frac{1}{N}, \quad \forall j \in \{1, \dots, N\}
\end{equation}
When a user departs, a vehicle is removed from $\mathbf{x}_t$, and a trip event is pushed to the trip heap $\Psi$.

\subsubsection{Trip Completion}
Trips scheduled to arrive at time $t$ are processed. For a trip arriving at $j$ with energy consumption $\delta_{soc}$:
\begin{equation}
  x_{j,t} \leftarrow \min(x_{j,t} + 1, C_j)
\end{equation}
If $x_{j,t} = C_j$, the station is full and the system attempts to reroute the user to the nearest available station.

\subsubsection{Relocation Actions}
Following a relocation plan, an operator moves scooters between stations, updating stocks and energy mass accordingly. The relocation cost is computed as:
\begin{equation}
  R_t = \sum_{(i,j,k) \in \text{Plan}_t} k \cdot d_{ij}
\end{equation}
where $k$ is the number of scooters relocated from station $i$ to $j$, constrained by available stock at $i$ and capacity at $j$.

\subsubsection{Charging Logic}
Charging is applied to idle scooters connected to plugs according to a charging plan. The number of charging scooters $k^{plug}_{i,t}$ is constrained by station stock $x_{i,t}$ and available chargers $K_i$. The energy mass update is:
\begin{equation}
  m_{i,t} \leftarrow m_{i,t} + k^{plug}_{i,t} \cdot \rho_i \cdot \Delta t
\end{equation}


\subsubsection{Optimization Objective}
% TODO: CHECK NOMENCLATURE WITH WHAT IS USED ABOVE (LoS, etc.)
The goal is to minimize the cumulative operational cost over the $H$ being formally defined as minimizing $\sum_{t=0}^{T} J_t$, where the step-wise cost $J_t$, corresponding to the decision criteria used to evaluate different agents and policies controlling the system, is:

\begin{equation}
  J_t = w_{U} \frac{U_t}{U_s} + w_{R} \frac{R_t}{R_s} + w_{C} \frac{C_t}{C_s} + w_{Q} \frac{Q_t}{Q_s}
\end{equation}

The cost components correspond to the Key Performance Indicators (KPIs) used to evaluate system performance.
\begin{itemize}
  \item $U_t$: \textbf{Unavailability Rate}. Defined as $1 - \frac{served\_new_t}{demand_t}$, representing the proportion of new arrivals that could not be served immediately during step $t$.
  \item $R_t$: \textbf{Relocation Distance}. The total kilometers traveled by the operator's fleet to rebalance scooters.
  \item $C_t$: \textbf{Charging Cost}. The financial cost (€) incurred by charging scooters, calculated based on energy consumed (kWh) and unit electricity price.
  \item $Q_t$: \textbf{Queue Backlog}. The total number of users currently waiting for scooters across all stations. Unlike $U_t$, which measures instantaneous failure, $Q_t$ penalizes persistent congestion.
\end{itemize}

The coefficients $w_{(\cdot)}$ represent the importance weights of each objective. To ensure numerical stability and comparable magnitudes between disparate units (ratios, kilometers, Euros, and user counts), each term is normalized by a baseline scale factor ($U_s, R_s, C_s, Q_s$).

\subsection{Methods}

In this section, the implementation choices for the agent-environment interaction loop are described. However, a distinction must first be made between \textit{operation policies} (or planners) and \textit{control policies} (learned policies). They represent connected but distinct layers of decision-making within the system, resulting in confusion regarding their similar terminology. To solve this, a renaming of the former was made.

\subsubsection{Operation Policies vs. Control Policies}

Scooter relocation and charging strategies, with which the agent intervenes in the system, take the form of a list of movements of $k$ scooters between station $i$ and $j$, and the number of scooters to be charged at each station, respectively. These plans are generated by algorithms referred to as \textit{operation policies}, but more distinctly as \textit{planners}. The planners are heuristic algorithms with explicitly defined rules, in contrast to \textit{control policies} learned through Reinforcement Learning (RL) techniques, which adapt based on experience and feedback from the environment, while mapping observations (system states) to actions.

Furthermore, the agent's action vector contains the parameters used by the planners to generate these relocation and charging plans, $\mathbf{a}_t = [a_0, a_1, a_2, a_3] \in [0,1]^4$. Therefore, although the generated plans directly affect system behavior, they are not an independent control policy, transforming observations into actions. Instead, they represent an intermediate abstraction layer that heuristically translates the agent's actions into executable interventions in the environment.

As the decision-making authority and optimization objective remain with the agent's learned control policy, this renaming separates the two concepts. It clarifies that planners are not granted the same level of control, even though they derive their actions from the agent's outputs and may appear similar at a nomenclatural level.

\subsubsection{\label{sec:agent_env}Agent-Environment Interaction}

The simulation environment's was implemented according to a standard reinforcement learning interface, similar to that of Gymnasium~\cite{towers2024gymnasium}. This was achieved through the implementation of the \texttt{reset} and \texttt{step} methods, which initialize the environment and apply an action, respectively, as it provides better modularity and separation of concerns. This design choice allows for near-seamless integration with RL libraries, only requiring the wrapping of the environment to match the expected input and output data structures' formats.

On the agent's actions, the previously explained abstraction from concrete plans to normalized parameters is crucial. It generalizes and simplifies the agent's decision-making process, allowing it to focus on high-level strategy rather than low-level operational details, enabling the use of continuous control algorithms that can efficiently explore and optimize within this bounded action space.

As a result, the overall interaction loop at each time step $t$ is as follows: the agent selects an action $\mathbf{a}_t$ according to its control policy $\pi(\mathcal{S}_t)$, either including the system state $\mathcal{S}_t$ as input or not, as will be detailed in \ref{sec:agents}. Each of the four components of the action vector is mapped to the specific parameters of the relocation and charging planners being used, which generate the respective plans. These plans are executed within the simulation environment, leading to a state transition to $\mathcal{S}_{t+1}$, and the agent receives a reward based on the defined objective function. This loop continues until the end of the simulation horizon, allowing the agent to learn and adapt its control policy over time.

\subsubsection{Planners Implemented}

\paragraph{Relocation Planners}

\begin{itemize}
  \item \textbf{Greedy Relocation}: Identifies stations with surplus of scooters (fill ratio $> \texttt{high}$) and deficit stations (fill ratio $< \texttt{low}$), then iteratively moves scooters to balance toward a \texttt{target} fill ratio. Donor stations are prioritized by proximity. The action components map to: $a_0 \rightarrow \texttt{low} \in [0.1, 0.3]$ and $a_1 \rightarrow \texttt{high} \in [0.6, 0.9]$, $a_2 \rightarrow \texttt{target} \in [0.4, 0.8]$.
  \item \textbf{Budgeted Relocation}: Extends greedy logic with a hard constraint on total relocation distance (\texttt{km\_budget}), explicitly capping operational costs per time step. This prevents excessive redistribution in cost-sensitive scenarios. By adding complexity to the plan generation, it is used for comparison with greedy relocation.
  \item \textbf{No-op Relocation}: Returns an empty plan, serving as a baseline to quantify the impact of active rebalancing.
\end{itemize}

\paragraph{Charging Planners}

\begin{itemize}
  \item \textbf{Greedy Charging}: Prioritizes stations with high demand and low SoC using score $\lambda_i (1 - s_i)$, allocating plugs in descending order of urgency. The agent controls the fraction of total charging capacity utilized via $a_3 \rightarrow \texttt{charge\_budget\_frac} \in [0.05, 0.40]$. This ensures scooters are charged where they are most needed.
  \item \textbf{Slack Charging}: Applies a demand-gating mechanism, only charging at stations below a demand quantile threshold, and prioritizing low SoC. This mitigates availability loss by concentrating charging at low-demand stations during off-peak periods. The goal was to approach the charging strategy from the opposite perspective of greedy charging, and compare their performances.
  \item \textbf{No-op Charging}: As a baseline, plugs zero scooters, isolating the effect of charging decisions in comparative experiments.
\end{itemize}

\subsubsection{\label{sec:agents}Agents Implemented}

The system evaluates four distinct agents, each representing a different approach to learning planner parameters within the bounded action space $\mathbf{a}_t \in [0,1]^4$.

\paragraph{Heuristic Agent}

A rule-based reactive controller that computes actions from observation features without learning. It maintains exponentially weighted moving averages (EWMA) of recent relocation distance and queue rate to detect operational trends. Actions are derived through explicit logic and change every simulation tick: relocation thresholds ($a_0, a_1$) adapt to queue pressure and capacity imbalance, the target fill ratio ($a_2$) responds to peak-hour detection and system throttling, and charging intensity ($a_3$) scales with low-SoC tail percentiles. This agent represents white-box decision-making, through manually crafted rules based on domain knowledge, providing interpretability and absolute transparency, but lacking adaptability. As such, it serves as a benchmark to evaluate the benefits of learning-based approaches.

\paragraph{Episode-Level Bandit (UCB1)}

A stateless multi-armed bandit that selects discrete planner parameter combinations in the form a fixed action (arm), that is selected every episode. The agent applies Upper Confidence Bound (UCB1) selection, balancing exploitation of high-performing arms with exploration of untried configurations. After all episodes are run, the agent converges to the arm with the highest average reward. As a very simple RL approach, it serves as a baseline to analyze the benefits of context-aware and continuous control agents.

\paragraph{Contextual Bandit with Receding Horizon (LinUCB)}

A context-aware bandit that selects arms within each simulation episode based on aggregated system features. The agent extracts a 16-dimensional context vector from observations (fill ratio statistics, SoC percentiles, queue metrics, time-of-day) and applies disjoint LinUCB with per-arm linear models. Arms correspond to discrete actions, reselected every \texttt{block\_minutes}. This receding-horizon strategy enables intra-episode adaptation to evolving conditions (e.g., shifting from low to high charging budget as SoC degrades), while maintaining exploration through ridge-regularized least-squares updates. The use of context and more frequent decision-making helps the agent respond quickly to changing conditions, while still learning effectively from limited data. This makes it a good choice for environments such as this project's, where demand patterns change over time and simple reactive strategies are not enough.

\paragraph{Soft Actor-Critic (SAC)}

A Deep RL agent that learns a continuous control policy $\pi(\mathbf{o}_t) \rightarrow \mathbf{a}_t$ through off-policy actor-critic optimization. Observations are flattened into a single vector from per-station state and temporal features. The agent can use frame-skipping, reducing decision frequency to stabilize learning in long-horizon episodes and counteract noise from the stochastic processes. SAC's entropy-regularized objective encourages exploration while maximizing cumulative reward, enabling it to discover different action sequences that balance service quality and operational cost across the different scenarios. This model was chosen for its strong performance in continuous control tasks, and being able to capture relationships such as the trade-offs between relocation aggressiveness and charging intensity, which, with a balance between exploration and exploitation, is expected to yield superior results in a complex, stochastic environment such as this one.

\subsubsection{Implementation Tools}
The experimental simulation was implemented in Python, using the following libraries:
\begin{itemize}
  \item \textbf{Gymnasium}~\cite{towers2024gymnasium}: The environment follows closely the standard Gymnasium interface, exposing \texttt{observation\_space} (state representation observed) and \texttt{action\_space} (possible actions to apply) to facilitate the integration of RL agents.
  \item \textbf{StableBaselines3}~\cite{stable-baselines3}: Used for implementing advanced learning-based controllers, specifically the Soft Actor-Critic (SAC) agent used for continuous control experiments.
  \item \textbf{NumPy}~\cite{harris2020array}: For high-performance vectorized state updates, numerical computations and stochastic generations.
  \item \textbf{NetworkX}~\cite{SciPyProceedings_11}: For graph traversal, distance matrix computation, and managing network connectivity.
\end{itemize}

\subsection{\label{sec:environ_spec}External Data}
The simulation environment is grounded in realistic topological data. The road network is extracted using OSMnx~\cite{https://doi.org/10.1111/gean.70009} from real cities, specifically selecting the bike-accessible network, as it most closely matches the one used by electric scooters. Each node is connected to every other node, with the edges retaining the shortest-path distance between them. This is done to abstract away the minute details of routing, which provide little meaning to the problem at hand, while preserving realistic inter-station distances. The travel time between stations is derived from these distances, assuming a constant average speed of 15 km/h, typical for shared electrical scooters in urban settings, and within the speed limits defined in Portuguese traffic legislation~\cite{DL102B2020}.


\subsection{Solution Design and Experimental Setup}

The simulation experiments were conducted using several environment configurations, each defined in a dedicated YAML file. Table~\ref{tab:env-configs} summarizes the main parameters for all tested scenarios.

\begin{table}[ht]
  \centering
  \caption{Summary of environment configurations used in experiments.}
  \label{tab:env-configs}
  \begin{threeparttable}
    \begin{tabular}{lcccc}
      \hline
      \textbf{Config} & \textbf{Porto10\tnote{a}} & \textbf{Porto20\tnote{b}} & \textbf{Porto20\tnote{c}} & \textbf{Lisbon20\tnote{d}} \\
      \hline
      Stations, $N$ & 10 & 20 & 20 & 20 \\
      Time step, $\Delta t$ & 5 & 5 & 5 & 5 \\
      Horizon, $H$ & 168 & 168 & 168 & 168 \\
      Capacity p/station, $C$ & 12 & 12 & 12 & 12 \\
      Chargers p/station, $K$ & 12 & 12 & 12 & 12 \\
      Charge rate, $\rho$ & 0.35 & 0.35 & 0.35 & 0.35 \\
      Battery, $B$ & 0.50 & 0.50 & 0.50 & 0.50 \\
      Energy cost, $c_{energy}$ & 0.20 & 0.20 & 0.20 & 0.20 \\
      Base demand, $\lambda$ & 0.35 & 0.35 & 0.35 & 0.35 \\
      Seed & 42 & 42 & 42 & 42 \\
      \hline
      \multicolumn{5}{l}{\textit{Scoring weights}} \\
      $w_\text{unavail}$ & 5.0 & 5.0 & 5.0 & 5.0 \\
      $w_\text{reloc}$ & 1.0 & 1.0 & 1.0 & 1.0 \\
      $w_\text{charge}$ & 1.0 & 1.0 & 1.0 & 1.0 \\
      $w_\text{queue}$ & 4.0 & 4.0 & 4.0 & 4.0 \\
      \hline
      \multicolumn{5}{l}{\textit{Normalization scales}} \\
      $U_0$ (unavail.) & 0.484 & 0.720 & 0.746 & 0.250 \\
      $R_0$ (reloc. km) & 0.809 & 0.965 & 0.816 & 1.00 \\
      $C_0$ (charge €) & 0.00664 & 0.0279 & 0.0273 & 0.0100 \\
      $Q_0$ (queue) & 49.13 & 205.71 & 234.70 & 10.00 \\
      \hline
    \end{tabular}
    \begin{tablenotes}\footnotesize
      \item Minimum distance between any pair of stations:
      \item[a] 500m
      \item[b] 600m
      \item[c] 900m
      \item[d] 700m
    \end{tablenotes}
  \end{threeparttable}
\end{table}

Additionally, all configurations include the full distance and travel time matrices, as well as the geographic coordinates and identifiers (IDs) for each station.

Four scenarios were designed to evaluate agent robustness under varying demand conditions:

\begin{itemize}
  \item \textbf{Baseline}: Homogeneous demand with standard time-of-day variation across all stations, representing typical operational conditions with uniform $\lambda_i$ and regular diurnal patterns.
  \item \textbf{Hotspot OD}: Introduces spatial demand heterogeneity designating specific stations as popular destinations, increasing their incoming trip probabilities in the origin-destination matrix $P$. This simulates real-world hotspots like commercial centers or tourist attractions that attract disproportionate demand.
  \item \textbf{Heterogeneous Lambda}: Assigns non-uniform base arrival rates $\lambda_i$ across stations, modeling intrinsic differences in station popularity due to location attributes (e.g., transit hubs vs. residential areas).
  \item \textbf{Event Heavy}: Applies stochastic event multipliers $f_{event}(t)$ on top of baseline demand, emulating sudden surges from concerts, sports matches, or weather disruptions that stress system capacity and charging reserves. Note that this does not increase demand on stations that are not affected by events.
\end{itemize}

% TODO:: Describe the experimental protocol, including the number of episodes, evaluation metrics, and statistical analysis methods used to compare agent performances across different scenarios.

\section{Results and Discussion}

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Methods to collect results
    \item Result presentations (graphs, charts, diagrams, tables,
    associations)
    \item Critical discussion of bad results
    \item Critical discussion of good results
    \item Focus on counter-intuitive results
    \item Focus on additional results (other than the rest of the literature)
  \end{itemize}
}

\section{Conclusion and Future Work}
\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Remark the conclusions drawn from the related work and gap analysis.
    \item Remark problem and goals.
    \item Remark the main results and findings.
    \item Summarize the main contributions.
    \item Scientific
    \item Application
    \item Technological
    \item Future work
    \item Further developments (how to improve the current work)
    \item Future opportunities / R\&D paths (spin-off projects/problems of the current work)
  \end{itemize}
}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
