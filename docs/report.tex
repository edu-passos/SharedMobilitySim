\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}

\begin{document}

\title{
  Shared Mobility Simulation Report\\
  {\footnotesize {A detailed analysis of agents to optimize operational cost and service}}
}

\author{
  \IEEEauthorblockN{Eduardo Passos}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202205630@up.pt
  }
  \and
  \IEEEauthorblockN{Guilherme Silva}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202205298@up.pt
  }
  \and
  \IEEEauthorblockN{Valentina Cadime}
  \IEEEauthorblockA{
    \textit{MSc in Artificial Intelligence} \\
    \textit{University of Porto}\\
    Porto, Portugal \\
    up202206262@up.pt
  }
}


\maketitle

\begin{abstract}
The following report involves the creation of a realistic simulation of an urban micro-mobility system, specialized for the management of shared electric scooter fleets. Its main purpose is to study different agents and respective policies under multiple scenarios, and, thus, analyze trade-offs between availability, waiting queues, and operational cost. The results indicate that Soft-Actor Critic (SAC) algorithm consistently delivers the best results across all scenarios. Nevertheless, its lack of interpretability may introduce risk and reduce transparency when implemented in the real world. Interpretable agents such as the Heuristic Agent (with manually crafted rules) and the Contextual Bandit (LinUCB) perform well, although they fail under demand heterogeneity and tend to sacrifice service quality, respectively. [PRECISO DE INFORMAÇÕES SOBRE O Episodic Multi-Armed Bandit]


\end{abstract}

\begin{IEEEkeywords}
Simulation, scooter fleet management, optimization, data analysis
\end{IEEEkeywords}

\section{Introduction}
Shared electric micro-mobility systems operated by private companies have become common in urban environments. This trend is driven by the daily congestion, environmental pressures, and the limited flexibility of traditional public transportation. Electric scooters offer a compact alternative for short-distance travel, and their deployment does not require major infrastructure investments. However, the management of these fleets presents the challenge of keeping scooters continuously available, which leads to costs related to scooter redistribution and battery charging, both of which are strongly impacted by spatial and temporal demand imbalance.

In more detail, the variance in scooter demand is influenced by commute patterns, weather conditions, and special events. As a result, in certain situations, some stations may become saturated while others are empty, which causes a reduced scooter availability and poor service quality. This imbalance is further complicated by battery constraints and charging delays, which limit vehicle usability and introduce additional operational dependencies. Since fleet size, charging capacity, and relocation resources are finite, operators must constantly decide when and where to relocate and recharge the scooters. On the one hand, an excessive intervention will increase operational cost, but on the other hand, an insufficient intervention leads to unmet demand, longer waiting times, and revenue loss. Ultimately, the problem consists of selecting redistribution and charging actions over time that minimize operational cost while maintaining acceptable service quality under uncertainty and resource constraints.

Indeed, an ever-changing city is poorly suited to simple static planning, as manually crafted rules do not account for demand heterogeneity and stochastic external influences. Consequently, there is a strong need for systematic evaluation tools that allow the comparison of management strategies and the analysis of trade-offs between availability, waiting queues, and operational cost. Simulation provides a controlled and flexible environment to study these interactions, avoiding the risks and expenses of real-world experimentation.

The aim of this project is to build a realistic simulation of an urban shared electric scooter system for the evaluation of daily fleet management strategies. The primary goal is to minimize operating costs while maximizing service quantity and quality. To achieve this, the simulation models user demand, vehicle travel, battery depletion, charging processes, and operator-driven relocation actions, while incorporating stochastic exogenous factors such as weather and events. Multiple decision-making approaches are evaluated, including baseline policies, interpretable heuristic and bandit-based agents, and reinforcement learning methods such as Soft Actor-Critic (SAC).

The central research questions address how different management policies perform under heterogeneous and time-dependent demand, how battery and charging constraints affect system efficiency, and what trade-offs arise between service quality and operational cost. The underlying hypothesis is that learning-based agents achieve superior performance by adapting to complex demand patterns, while interpretable approaches provide greater transparency but may degrade under demand heterogeneity.

This report first describes the system model and simulation framework, followed by the decision-making agents and policies. The evaluation methodology and performance metrics are then presented, along with a comparative analysis of results across multiple scenarios. The report concludes with a discussion of key findings, limitations, and directions for future work.

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Context
    \item Problem statement. Clearly define the research question or technical challenge with a precise problem description and formal representations
    \item Motivation (to solve the problem) / significance
    \item Aim and goals
    \item Aim: the main expected outcome of this research project (the big picture!)
    \item Goals: specific objectives to be accomplished
    \item Research questions
    \item Hypotheses
    \item Document/paper structure
  \end{itemize}
}

\section{Related Work}

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Review of existing literature.
    \item Relevant concepts and studies.
    \item Summarize background and related work to highlight gaps addressed by the project.
    \item Discussion and critical analysis.
  \end{itemize}
}

The problem falls within the field of Urban Mobility and, more specifically, the subfield of Shared Micromobility and Fleet Management. It addresses how cities can efficiently manage and balance fleets of shared electric vehicles to meet fluctuating demand patterns throughout the day. As a result, and similarly to many of the modelling projects that seek to study the real world, this issue lies at the intersection of several disciplines.

From the perspective of transportation science, the problem concerns the modeling of human movement patterns and travel demand within urban environments. It involves understanding how people choose modes of transport, how their choices change with time of day, weather, or location, and how these behaviors influence the spatial distribution of shared vehicles.

In the area of optimization and operations research, the challenge translates into a set of complex decision problems. These include vehicle rebalancing (determining how and when to move vehicles from low-demand to high-demand areas), routing and scheduling of redistribution trucks, and the design of user incentives to encourage self-balancing behavior. Such problems are often formulated as variants of the Vehicle Routing Problem (VRP), capacitated network flow models, or stochastic optimization problems. Furthermore, the possibility of incorporating machine learning brings a predictive and adaptive layer to the system.


\section{Materials and Methods}

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Problem formalization: what to solve?
    \item Materials:
    \begin{itemize}
      \item Data
      \item Tools
      \item Techniques
    \end{itemize}
    \item Methods: how to solve it?
    \item Solution design
  \end{itemize}
}


The system is formalized as an agent-based discrete-time simulation shaped by the interactions between a single operator (the agent), the entities, and finite network resources. The agent acts by defining a 4-dimensional continuous action vector $\mathbf{a}_t = [a_0, a_1, a_2, a_3] \in [0,1]^4$, where each component corresponds to a specific parameter that influences scooter relocation and charging. The primary entities are the customers and the vehicle fleet. Customers are modeled as transient entities represented by stochastic arrival events, while the fleet consists of vehicle counts circulating between nodes. These entities interact with stations, which function as passive resources within the system. Stations are characterized by static attributes such as geographic location, parking capacity (slots), and charger availability, effectively constraining the flow of entities by limiting parking and charging capcities.

The simulation operates within a dynamic environment, whose specific instantiation for the experiments conducted is defined in \ref{sec:environ_spec}, subject to external variations, including time-of-day effects, weather conditions, and special events. These environmental factors, combined with the stochastic user demand, constitute the \textit{uncontrollable exogenous variables} which dictate the probabilistic part of the system, as detailed in \ref{sec:sim_dynamics}.

The instantaneous status of the system is captured by state variables, such as the number of vehicles, State of Charge (SoC) metrics, and the size of user queues at each station. These metrics correspond to the \textit{endogenous variables} formally defined in \ref{sec:sim_state}. The evolution of this state is driven by discrete events, including customer arrivals, trip completions, and operator interventions, which trigger specific activities including vehicle rental, fleet relocation, and battery charging. The infrastructure parameters and resource limits that bound these activities form the \textit{controllable exogenous variables}, explored in \ref{sec:sim_config}. Additionally, the specific relocation and charging plans built by the operational policies at each time step are also controllable exogenous variables, as they are generated externally to the simulation dynamics, by the agent.


% TODO: Establish differences between (so-called, according to MS slides) operation policies and ML policies (control policies). Completely different things.

\subsection{\label{sec:sim_config}Simulation Configuration}
The simulation environment is characterized by a set of controllable exogenous parameters (implemented in the \texttt{SimConfig} dataclass).
\begin{itemize}
  \item $\Delta t$: Discrete simulation time step duration (minutes).
  \item $H$: Total simulation horizon (hours).
  \item $C \in \mathbb{N}^N$: Vector of station capacities, where $C_i$ is the maximum number of vehicles allowed at station $i$.
  \item $\mathcal{T} \in \mathbb{R}^{N \times N}$: Travel time matrix, where $\tau_{ij}$ represents the effective transit time between station $i$ and $j$.
  \item $\mathcal{D} \in \mathbb{R}^{N \times N}$: Relocation cost matrix, where $d_{ij}$ represents the distance or cost coefficient for moving vehicles between stations $i$ and $j$.
  \item $K \in \mathbb{N}^N$: Vector of charging station capacities (number of plugs) at each node.
  \item $\rho \in \mathbb{R}^N$: Charging rate vector, where $\rho_i$ denotes the SoC gain per hour at station $i$.
  \item $B$: Total battery capacity per vehicle (kWh).
  \item $c_{energy}$: Unit cost of energy (€/kWh).
  \item $s_{min}$: Minimum SoC threshold required for a vehicle to be eligible for rental.
  \item $\gamma_{res} \in \{0,1\}$: Operation policy parameter indicating if vehicles connected to the grid should be reserved (unavailable for rental) during the active charging time step.
\end{itemize}

\subsection{\label{sec:sim_state}Simulation State}
At any time $t$, the system state is defined by endogenous variables tracking the fleet and demand status (implemented as attributes of the \texttt{Sim} class):
\begin{itemize}
  \item $\mathbf{x}_t \in \mathbb{N}^N$: Vehicle count at each station.
  \item $\mathbf{s}_t \in [0,1]^N$: Average SoC at each station.
  \item $\mathbf{m}_t \in \mathbb{R}^N$: Aggregated energy "mass" at each station. Calculated as $m_{i,t} = x_{i,t} \cdot s_{i,t}$.
  \item $\mathbf{q}_t \in \mathbb{N}^N$: Number of users currently queuing for a vehicle.
  \item $\Psi_t$: Set of active trips currently in transit, implemented as a min-heap priority queue sorted by arrival time. A trip is a tuple $(t_{arrival}, j, \delta_{soc}, s_{depart})$, denoting arrival time, destination station, SoC usage, and departure SoC.
\end{itemize}

\subsection{\label{sec:sim_dynamics}Simulation Dynamics}

The state transition $\mathcal{S}_t \rightarrow \mathcal{S}_{t+1}$ occurs via the following sequential logic step, in which the uncontrollable exogenous variables manifest as stochastic events:

\subsubsection{Demand and Departures}
Demand arrivals $\mathbf{A}_t$ follow a non-homogeneous Poisson process. A vehicle at station $i$ is rentable only if it satisfies the minimum SoC threshold ($s_{i,t} > s_{min}$). As such, the number of served users, $d_{out}^i$, is limited by the rentable stock.
User destinations are sampled from the origin-destination probability matrix $P \in \mathbb{R}^{N \times N}$, where $p_{ij}$ represents the probability of a user departing from station $i$ terminating their trip at station $j$. To simplify the simulation and focus on rebalancing dynamics rather than complex routing patterns, we assume a uniform distribution:
\begin{equation}
    p_{ij} = \frac{1}{N}, \quad \forall j \in \{1, \dots, N\}
\end{equation}
This implies that a user picking up a vehicle at any station is equally likely to ride to any other station in the network. When a user departs, a vehicle is removed from $\mathbf{x}_t$, and a trip event is pushed to the event heap $\Psi$.

\subsubsection{Trip Completion}
Trips scheduled to arrive at time $t$ are processed. For a trip arriving at $j$ with energy consumption $\delta_{soc}$:
\begin{equation}
    x_{j,t} \leftarrow \min(x_{j,t} + 1, C_j)
\end{equation}
If $x_{j,t} = C_j$, the station is full and the system attempts to reroute the user to the nearest available station $k$.

\subsubsection{Relocation Actions}
Following a relocation plan, an operator moves vehicles between stations, updating stocks and energy mass accordingly, with the goal of balancing the fleet distribution.

\subsubsection{Charging Logic}
Charging is applied to idle vehicles connected to plugs according to a charging plan. The number of charging vehicles $k^{plug}_{i,t}$ is constrained by station stock $x_{i,t}$ and available chargers $K_i$. The energy mass update is:
\begin{equation}
    m_{i,t} \leftarrow m_{i,t} + k^{plug}_{i,t} \cdot \rho_i \cdot \Delta t
\end{equation}


\subsubsection{Optimization Objective}
% TODO: CHECK NOMENCLATURE WITH WHAT IS USED ABOVE (LoS, etc.)
The goal is to find a control policy $\pi(\mathcal{S}_t) \rightarrow \mathcal{A}_t$, comprising relocation decisions and charging schedules, that minimizes the cumulative operational cost over the horizon $T$. The objective function represents a trade-off between Level of Service (LoS) and operational expenditure. As such, this is the decision criteria used to evaluate different agents and policies controlling the system. The optimization problem is formally defined as minimizing $\sum_{t=0}^{T} J_t$, where the step-wise cost $J_t$ is:

\begin{equation}
  J_t = w_{U} \frac{U_t}{U_s} + w_{R} \frac{R_t}{R_s} + w_{C} \frac{C_t}{C_s} + w_{Q} \frac{Q_t}{Q_s}
\end{equation}

The cost components correspond to the following system metrics:
\begin{itemize}
  \item $U_t$: \textbf{Unavailability Rate}. Defined as $1 - \frac{served\_new_t}{demand_t}$, representing the proportion of new arrivals that could not be served immediately.
  \item $R_t$: \textbf{Relocation Distance}. The total kilometers traveled by the operator's fleet to rebalance vehicles during step $t$.
  \item $C_t$: \textbf{Charging Cost}. The financial cost (€) incurred by charging vehicles, calculated based on energy consumed (kWh) and unit electricity price.
  \item $Q_t$: \textbf{Queue Backlog}. The total number of users currently waiting for vehicles across all stations. Unlike $U_t$, which measures instantaneous failure, $Q_t$ penalizes persistent congestion.
\end{itemize}

The coefficients $w_{(\cdot)}$ represent the importance weights of each objective. To ensure numerical stability and comparable magnitudes between disparate units (ratios, kilometers, Euros, and user counts), each term is normalized by a baseline scale factor ($U_s, R_s, C_s, Q_s$).

% TODO: Update KPIs according to the ones we are actually using.
The performance is measured by \textbf{Output Variables (KPIs)}, as detailed in Table \ref{tab:kpis}.

\begin{table}[htbp]
  \caption{Key Performance Indicators}
  \begin{center}
    \begin{tabular}{|l|p{3.5cm}|l|}
      \hline
      \textbf{KPI} & \textbf{Meaning} & \textbf{Goal} \\
      \hline
      Availability & Fraction of stations with at least one vehicle & Maximize \\
      \hline
      Unmet demand & Number of users who could not find a scooter & Minimize \\
      \hline
      Standard deviation & Measure of imbalance across the network & Minimize \\
      \hline
      Scooters in use & Vehicles currently rented or moving & Maximize \\
      \hline
      Idle station time & Time periods when a station has no activity & Minimize \\
      \hline
      Relocation distance & Total distance traveled during rebalancing (km) & Minimize \\
      \hline
      Energy cost & Cumulative electricity cost for charging & Minimize \\
      \hline
      Success rate & Ratio of served users to total demand & Maximize \\
      \hline
    \end{tabular}
  \label{tab:kpis}
  \end{center}
\end{table}

\subsection{Methods}

With the creation and formalization of the simulation environment, it is important to concretely describe the interactions between the agent and the system. However, a distinction must first be made between \textit{operation policies} (or planners) and \textit{control policies} (learned policies), as they represent connected, but distinct layers of decision-making within the system, and the similar terminology can lead to confusion.

Scooter relocation and charging strategies, with which the agent intervenes in the system, take the form of a list of movements of $k$ scooters between station $i$ and $j$, and the number of scooters to be charged at each station, respectively. These plans are generated by algorithms referred to as \textit{operation policies}, but more distinctly as \textit{planners}. The planners are heuristic algorithms with explicitly defined rules, in contrast to \textit{control policies} learned through Reinforcement Learning (RL) techniques, which adapt based on experience and feedback from the environment, and map observations (system states) to actions.

Furthermore, as described previously, the agent's action vector $\mathbf{a}_t$ dictates the parameters used by the planners to generate these relocation and charging plans. Therefore, although the generated plans directly affect system behavior, they are not an independent control policy. Instead, they represent an intermediate abstraction layer that heuristically (and deterministically) translates the agent's actions into executable interventions in the environment. As the decision-making authority and optimization objective remain with the agent's learned control policy, this renaming separates the two concepts. It clarifies that planners are not granted the same level of control, even though they derive their actions from the agent's outputs and may appear similar at a nomenclatural level.

% TODO: Update
Baseline policies include:
\begin{itemize}
  \item \textbf{Greedy Rebalancing}: Moving scooters from overfilled stations ($x > 0.8C$) to depleted ones ($x < 0.2C$).
  \item \textbf{Nightly Uniform Rebalancing}: Resetting stations to a uniform fill level ($\approx 60\%$) at 02:00.
  \item \textbf{Random Demand Scenario}: A neutral benchmark without relocation.
\end{itemize}

\subsubsection{Implementation Tools}
The experimental simulation was implemented in \textbf{Python}, using the following libraries:
\begin{itemize}
  \item \textbf{NumPy}~\cite{harris2020array}: For high-performance vectorized state updates, numerical computations and stochastic generations.
  \item \textbf{NetworkX}~\cite{SciPyProceedings_11}: For graph traversal, distance matrix computation, and managing network connectivity.
  \item \textbf{Gymnasium}~\cite{towers2024gymnasium}: The environment adheres to the standard Gym interface, exposing \texttt{observation\_space} (state representation observed) and \texttt{action\_space} (possible actions to apply) to facilitate the integration of RL agents.
  \item \textbf{StableBaselines3}~\cite{stable-baselines3}: Used for implementing advanced learning-based controllers, specifically the Soft Actor-Critic (SAC) agent used for continuous control experiments.
\end{itemize}

\subsection{\label{sec:environ_spec}External Data}
The simulation environment is grounded in realistic topological data. The road network layer is extracted using OSMnx~\cite{https://doi.org/10.1111/gean.70009} for the city of Porto, Portugal, from which a station graph is constructed. Each node is connected to every other node, with the edges retaining the shortest-path distance between them. This is done to abstract away the minute details of routing, which provide little meaning to the problem at hand, while preserving realistic inter-station distances. The travel time between stations is also included in the graph's edge informationl, computed using the distance and average speed of a shared electric scooter.

% TODO: Add this somewhere:
% assuming a constant average speed of 15 km/h, typical for shared electrical scooters in urban settings, and within the speed limits defined in Portuguese traffic legislation \cite{DL102B2020}.



\subsection{Solution Design and Experimental Setup}

With the specific instatiation of the simulation environment generated and defined,



\section{Results and Discussion}

\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Methods to collect results
    \item Result presentations (graphs, charts, diagrams, tables,
    associations)
    \item Critical discussion of bad results
    \item Critical discussion of good results
    \item Focus on counter-intuitive results
    \item Focus on additional results (other than the rest of the literature)
  \end{itemize}
}

\section{Conclusion and Future Work}
\textit{
  \textbf{TODOs:}
  \begin{itemize}
    \item Remark the conclusions drawn from the related work and gap analysis.
    \item Remark problem and goals.
    \item Remark the main results and findings.
    \item Summarize the main contributions.
    \item Scientific
    \item Application
    \item Technological
    \item Future work
    \item Further developments (how to improve the current work)
    \item Future opportunities / R\&D paths (spin-off projects/problems of the current work)
  \end{itemize}
}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
